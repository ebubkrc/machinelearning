{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e812f9e",
   "metadata": {},
   "source": [
    "### Text generation with RNN example;\n",
    "\n",
    "* This code is mostly stolen from this website. Reference:https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0e1e6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the classes and functions that we needb for training our model\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5f62a9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163816\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "#dowloaded the text and put it into the same directory with python program\n",
    "#Read the data, turn it into lower case\n",
    "data = open(\"alice_in_wonderland.txt\",\"r\",encoding='utf-8').read().lower()\n",
    "\n",
    "\n",
    "#creating a set of all of the distinct characters in the text\n",
    "#This get the set of characters used in the data and sorts them\n",
    "# represent each character with a unique integer\n",
    "chars = sorted(list(set(data)))\n",
    "\n",
    "#Total number of characters used in the data\n",
    "totalChars = len(data)\n",
    "\n",
    "#Number of unique chars\n",
    "numberOfUniqueChars = len(chars)\n",
    "\n",
    "\n",
    "\n",
    "print(totalChars)     #the text has more than 150.000 characters\n",
    "print(numberOfUniqueChars)  #number of distinct characters more than alphabet 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e035756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This allows for characters to be represented by numbers\n",
    "CharsForids = {char:Id for Id, char in enumerate(chars)}\n",
    "\n",
    "#This is the opposite to the above\n",
    "idsForChars = {Id:char for Id, char in enumerate(chars)}\n",
    "\n",
    "#How many timesteps? or how many characters we want to process in one go\n",
    "numberOfCharsToLearn = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e58bd995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since our timestep sequence represents a process for every 100 chars we omit\n",
    "#the first 100 chars so the loop runs a 100 less or there will be index out of\n",
    "#range\n",
    "counter = totalChars - numberOfCharsToLearn\n",
    "\n",
    "#Inpput data\n",
    "charX = []\n",
    "#output data\n",
    "y = []\n",
    "#This loops through all the characters in the data skipping the first 100\n",
    "for i in range(0, counter, 1):\n",
    "    #This one goes from 0-100 so it gets 100 values starting from 0 and stops\n",
    "    #just before the 100th value\n",
    "    theInputChars = data[i:i+numberOfCharsToLearn]\n",
    "    #With no : you start with 0, and so you get the actual 100th value\n",
    "    #Essentially, the output Chars is the next char in line for those 100 chars\n",
    "    #in X\n",
    "    theOutputChars = data[i + numberOfCharsToLearn]\n",
    "    #Appends every 100 chars ids as a list into X\n",
    "    charX.append([CharsForids[char] for char in theInputChars])\n",
    "    #For every 100 values there is one y value which is the output\n",
    "    y.append(CharsForids[theOutputChars])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4084bcfb",
   "metadata": {},
   "source": [
    "###### First we must transform the list of input sequences into the form [samples, time steps, features] expected by an LSTM network.\n",
    "###### Next we need to rescale the integers to the range 0-to-1 to make the patterns easier to learn by the LSTM network that uses the sigmoid activation function by default.\n",
    "###### Finally, we need to convert the output patterns (single characters converted to integers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b9ea37f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.reshape(charX, (len(charX), numberOfCharsToLearn, 1))\n",
    "#Len charX represents how many of those time steps we have\n",
    "#Our features are set to 1 because in the output we are only predicting 1 char\n",
    "#Finally numberOfCharsToLearn is how many character we process\n",
    "\n",
    "\n",
    "#This is done for normalization\n",
    "X = X/float(numberOfUniqueChars)\n",
    "\n",
    "#This sets it up for us so we can have a categorical(#feature) output format\n",
    "y = np_utils.to_categorical(y)\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aeede7",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd0d69be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1280/1280 [==============================] - 827s 616ms/step - loss: 3.0980\n",
      "Epoch 2/5\n",
      "1280/1280 [==============================] - 457s 357ms/step - loss: 2.8475\n",
      "Epoch 3/5\n",
      "1280/1280 [==============================] - 440s 343ms/step - loss: 2.7560\n",
      "Epoch 4/5\n",
      "1280/1280 [==============================] - 422s 330ms/step - loss: 2.6780\n",
      "Epoch 5/5\n",
      "1280/1280 [==============================] - 361s 282ms/step - loss: 2.6200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e189c74040>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#Since we know the shape of our Data we can input the timestep and feature data\n",
    "#The number of timestep sequence are dealt with in the fit function\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))     #LSTM layer with 256 memory units\n",
    "model.add(Dropout(0.2))   #A Dropout layer to prevent overfitting to the training data\n",
    "\n",
    "\n",
    "#number of features on the output\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ab1729",
   "metadata": {},
   "source": [
    "There is no test dataset. We are modeling the entire training dataset to learn the probability of each character in a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6606c04",
   "metadata": {},
   "source": [
    "##### finally, make predictions\n",
    "  *  ###### pick a random input pattern as our seed sequence, then print generated characters as we generate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7d0be21e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‘or perhaps they won’t walk the way i want\n",
      "to go! let me see: i’ll give them a new pair of boots ev\n"
     ]
    }
   ],
   "source": [
    "randomVal = np.random.randint(0, len(charX)-1)\n",
    "randomStart = charX[randomVal]\n",
    "print(\"\".join([idsForChars[value] for value in randomStart]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f04a000f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o the tooee to the tooee to the tooee to the tooee to the tooee to the tooee to the tooee to the too\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    \n",
    "    x = np.reshape(randomStart, (1, len(randomStart), 1))\n",
    "    x = x/float(numberOfUniqueChars)\n",
    "    pred = model.predict(x, verbose=0)\n",
    "    index = np.argmax(pred)\n",
    "    randomStart.append(index)\n",
    "    randomStart = randomStart[1: len(randomStart)]\n",
    "    \n",
    "print(\"\".join([idsForChars[value] for value in randomStart]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e10cd5",
   "metadata": {},
   "source": [
    "I couldnt be able to generate a meaningful sentence text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4e29b6",
   "metadata": {},
   "source": [
    "#### as a summary wa can list the steps of our approach like;\n",
    "* Convert text from list of strings into list of lists of integers (sequences)\n",
    "* Create feature and labels from sequences\n",
    "* Build LSTM model with LSTM, and Dense layers\n",
    "* Train model to predict next work in sequence\n",
    "* Make predictions by passing in starting sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b455ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
