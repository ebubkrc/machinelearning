{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    " * What I understood from RNNs? \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the course, we saw how convolutional feed forward neural networks work on handwritten digit recognition (mnist dataset) and object  classification (fashion mnist dataset).  We also saw traiditional feed forward neural networks. They both are really good at classification. \n",
    "\n",
    "##### So, what's the problem?\n",
    "\n",
    "* Although their good successes, feed forward neural networks are still very limited in what they can achieve.  In feed forward nets we were recognizing indivisual sample by classificiation or object detection. However, We can  also  analyze all input sequences in the same system.  In those layers sequences there are many useful information. These sequences may have complex and have different length of information. \n",
    "* For example speech recognition, vision are changing over time and as a result they produce high dimensional data. Proccessing this data with feed forward nets is not possible or they are not goot at modelling. \n",
    "\n",
    "\n",
    "##### What is RNN?\n",
    "\n",
    "* For the problem that I have mentioned above, how to learn these sequences of informaion, recurrent neural network RNN is the solution. In the last lecture of the course we talked about memory based operations, feedback conventions and how neurons connected each other different from feed forward nets. \n",
    "\n",
    "* Feed forward neural networks were set in layers and information flows in one direction from input to output. It means that there were strict rules. I mean all connection are directed there is no undirected connection.\n",
    "* So, to make more powerful systems RNNs are allowed to break those rules. RNNs do not have to be organized in layers and actually neurons are allowed to be connected themselves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ca65cda94c8"
   },
   "source": [
    "# Recurrent Neural Networks (RNN) with Keras\n",
    "   * Reference: https://www.tensorflow.org/guide/keras/rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6873211b02d4"
   },
   "source": [
    "\n",
    "\n",
    "Recurrent neural networks (RNN) are a class of neural networks that is powerful for\n",
    "modeling sequence data such as time series or natural language.\n",
    "\n",
    "\n",
    "Schematically, a RNN layer uses a `for` loop to iterate over the timesteps of a\n",
    "sequence, while maintaining an internal state that encodes information about the\n",
    "timesteps it has seen so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3600ee25c8e"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T18:08:07.626882Z",
     "iopub.status.busy": "2021-03-05T18:08:07.626262Z",
     "iopub.status.idle": "2021-03-05T18:08:13.814281Z",
     "shell.execute_reply": "2021-03-05T18:08:13.813686Z"
    },
    "id": "71c626bbac35"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4041a2e9b310"
   },
   "source": [
    "## Built-in RNN layers: a simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98e0c38cf95d"
   },
   "source": [
    "There are three built-in RNN layers in Keras:\n",
    "\n",
    "keras.layers.SimpleRNN, a fully-connected RNN where the output from previous timestep is to be fed to next timestep.\n",
    "\n",
    "keras.layers.GRU\n",
    "\n",
    "keras.layers.LSTM\n",
    "\n",
    "Here is a simple example of a `Sequential` model that processes sequences of integers,\n",
    "embeds each integer into a 64-dimensional vector, then processes the sequence of\n",
    "vectors using a `LSTM` layer.\n",
    "* Ease of use: the built-in keras.layers.RNN, keras.layers.LSTM, keras.layers.GRU layers enable you to quickly build recurrent models without having to make difficult configuration choices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This code part is copied from official TensorFlow website. Reference: https://www.tensorflow.org/guide/keras/rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T18:08:13.823039Z",
     "iopub.status.busy": "2021-03-05T18:08:13.818885Z",
     "iopub.status.idle": "2021-03-05T18:08:22.834887Z",
     "shell.execute_reply": "2021-03-05T18:08:22.835389Z"
    },
    "id": "a5617759e54e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 64)          64000     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 164,106\n",
      "Trainable params: 164,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "# Add an Embedding layer expecting input vocab of size 1000, and\n",
    "# output embedding dimension of size 64.\n",
    "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
    "\n",
    "# Add a LSTM layer with 128 internal units.\n",
    "model.add(layers.LSTM(128))\n",
    "\n",
    "# Add a Dense layer with 10 units.\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the first line, we are using the Keras Sequential model like we used before in our cnn example. It means we build the network up one layer at a time.\n",
    "*  For the second line there is an  Embedding layer which maps each input to a 100-dimensional vector. For example if we use this RNN for the text generation, this layer maps the input word to a desired dimension. \n",
    "\n",
    "* At the center of an RNN there are layers composed of memory cells. LSTM consists of a memory cell which stores information for extended periods of time. \n",
    "\n",
    "* The LSTM has 3 different gates. there is a 'forget' gate for discarding irrelevant information, an “input” gate for using the current input, and an “output” gate for producing predictions at each time step. The function of each cell element is decided by the parameters (weights) which are learned during training.\n",
    "\n",
    "* A Dense fully-connected output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43aa4e4f344d"
   },
   "source": [
    "## Outputs and states\n",
    "\n",
    "###### Reference: https://www.tensorflow.org/guide/keras/rnn\n",
    "By default, the output of a RNN layer contains a single vector per sample. This vector\n",
    "is the RNN cell output corresponding to the last timestep, containing information\n",
    "about the entire input sequence. The shape of this output is `(batch_size, units)`\n",
    "where `units` corresponds to the `units` argument passed to the layer's constructor.\n",
    "\n",
    "A RNN layer can also return the entire sequence of outputs for each sample (one vector\n",
    "per timestep per sample), if you set `return_sequences=True`. The shape of this output\n",
    "is `(batch_size, timesteps, units)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### * If you want to use RNN to analyse continuous data (mostly used in), you must use batch_input_shape() function\n",
    "###### * In this function, there are 3 things you must provide batchsize, timestep, input_dim\n",
    "   * batchsize: the number of rows to proceed on each epoch\n",
    "    * timestep: the time interval of your data.\n",
    "     * input_dim: number of column you are going to put in LSTM\n",
    "\n",
    "The heart of the network: a layer of LSTM cells with dropout to prevent overfitting.\n",
    "A Dropout layer to prevent overfitting to the training data.\n",
    "\n",
    "The input to the LSTM layer is (None, 50, 100) which means that for each batch (the first dimension), each sequence has 50 timesteps (words), each of which has 100 features after embedding. Input to an LSTM layer always has the (batch_size, timesteps, features) shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T18:08:22.842884Z",
     "iopub.status.busy": "2021-03-05T18:08:22.842158Z",
     "iopub.status.idle": "2021-03-05T18:08:23.105050Z",
     "shell.execute_reply": "2021-03-05T18:08:23.105488Z"
    },
    "id": "c3294dec91e4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 64)          64000     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, None, 256)         247296    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 128)               49280     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 361,866\n",
      "Trainable params: 361,866\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
    "\n",
    "# The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256)\n",
    "model.add(layers.GRU(256, return_sequences=True))\n",
    "\n",
    "# The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)\n",
    "model.add(layers.SimpleRNN(128))\n",
    "\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e97a845a372a"
   },
   "source": [
    "## RNN layers and RNN cells\n",
    "\n",
    "In addition to the built-in RNN layers, the RNN API also provides cell-level APIs.\n",
    "Unlike RNN layers, which processes whole batches of input sequences, the RNN cell only\n",
    "processes a single timestep.\n",
    "\n",
    "The cell is the inside of the `for` loop of a RNN layer. Wrapping a cell inside a\n",
    "`keras.layers.RNN` layer gives you a layer capable of processing batches of\n",
    "sequences, e.g. `RNN(LSTMCell(10))`.\n",
    "\n",
    "Mathematically, `RNN(LSTMCell(10))` produces the same result as `LSTM(10)`. In fact,\n",
    "the implementation of this layer in TF v1.x was just creating the corresponding RNN\n",
    "cell and wrapping it in a RNN layer. \n",
    "\n",
    "There are three built-in RNN cells, each of them corresponding to the matching RNN\n",
    "layer.\n",
    "\n",
    "- `keras.layers.SimpleRNNCell` corresponds to the `SimpleRNN` layer.\n",
    "\n",
    "- `keras.layers.GRUCell` corresponds to the `GRU` layer.\n",
    "\n",
    "- `keras.layers.LSTMCell` corresponds to the `LSTM` layer.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "rnn.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
